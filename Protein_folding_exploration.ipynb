{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein_folding_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWAVvMI6ArpJLqJleiZQSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcummer/deep_learning_short_projects/blob/main/Protein_folding_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDhPFRLAanB"
      },
      "source": [
        "This is an exploratory project to play around with applying various NLP techniques to predict the secondary structure of a protein from the amino acid sequence. I compare performance to the foundational work in http://www.columbia.edu/~nq6/publications/protein.pdf which reported a best single model accuracy of 62.7% using a sliding window multilayer perceptron.\n",
        "\n",
        "There's a lot of directions to take this with likely highlights being deploying a proper NLP model like a biLSTM with randomly initialized embeddings for the amino acids, exploring those embeddings to see if they capture the similarities we'd expect i.e. arginine and lysine are close in that space, and incorporating biophysical properties of the amino acids such as charge and flags for special amino acids such as proline.\n",
        "\n",
        "We won't worry about optimizing the details of model architecture and training for the most part. If we end up with something particularly high performing we may dig into it. We're also not going to bother with a train-val-test split since this work is purely exploratory and the goal is to try interesting things not report rigorous performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK-hBf0ndpvf",
        "outputId": "0bff388d-e561-4cd0-8fe9-f9fa1ee9233e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download our dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.train\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-17 13:26:51--  https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.train\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73489 (72K) [application/x-httpd-php]\n",
            "Saving to: ‘protein-secondary-structure.train’\n",
            "\n",
            "protein-secondary-s 100%[===================>]  71.77K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-11-17 13:26:52 (543 KB/s) - ‘protein-secondary-structure.train’ saved [73489/73489]\n",
            "\n",
            "--2020-11-17 13:26:52--  https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14586 (14K) [application/x-httpd-php]\n",
            "Saving to: ‘protein-secondary-structure.test’\n",
            "\n",
            "protein-secondary-s 100%[===================>]  14.24K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-11-17 13:26:52 (220 KB/s) - ‘protein-secondary-structure.test’ saved [14586/14586]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qbPeYxeTwn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to load the text file and return a minimally formatted dataframe\n",
        "def parse_to_df(path):\n",
        "  with open(path,'r') as f:\n",
        "    content = f.readlines()\n",
        "  df = pd.DataFrame()\n",
        "  protein_count = 0\n",
        "  amino_acids = []\n",
        "  structure = []\n",
        "  for i in content:\n",
        "    i = i.strip()\n",
        "    if 'end' in i:\n",
        "        df = df.append(pd.DataFrame({'amino_acid':[amino_acids], 'structure':[structure], 'protein_count':[protein_count]}))\n",
        "        protein_count += 1\n",
        "        amino_acids = []\n",
        "        structure = []\n",
        "    elif len(i) == 3:\n",
        "      amino_acids.append(i.split(' ')[0])\n",
        "      structure.append(i.split(' ')[1])\n",
        "  return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYiBkP1H9cU9"
      },
      "source": [
        "Here we load the text files for the train and test splits into easily accessed dataframes. We also also assign an index to each amino acid and structure to replace the text character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcdRguigfH6z"
      },
      "source": [
        "# Simple data preparation: load the data from text files, find the unique set of amino acids, convert text characters to indices\n",
        "\n",
        "df_train_raw = parse_to_df('/content/protein-secondary-structure.train')\n",
        "df_test_raw = parse_to_df('/content/protein-secondary-structure.test')\n",
        "\n",
        "\n",
        "unique_amino_acids_in_train = np.unique([item for sublist in df_train_raw.amino_acid for item in sublist])\n",
        "unique_amino_acids_in_test = np.unique([item for sublist in df_test_raw.amino_acid for item in sublist])\n",
        "[unique_amino_acids_in_train.append(x) for x in unique_amino_acids_in_test if x not in unique_amino_acids_in_train]\n",
        "\n",
        "amino_acid_to_index = {}\n",
        "i=0\n",
        "for x in unique_amino_acids_in_train:\n",
        "  amino_acid_to_index[x] = i\n",
        "  i += 1\n",
        "\n",
        "\n",
        "structure_to_index = {'_': 0, 'h': 1, 'e': 2}\n",
        "\n",
        "df_train_raw['amino_acid_index'] = [[amino_acid_to_index[x] for x in y] for y in df_train_raw.amino_acid]\n",
        "df_test_raw['amino_acid_index'] = [[amino_acid_to_index[x] for x in y] for y in df_test_raw.amino_acid]\n",
        "df_train_raw['structure_index'] = [[structure_to_index[x] for x in y] for y in df_train_raw.structure]\n",
        "df_test_raw['structure_index'] = [[structure_to_index[x] for x in y] for y in df_test_raw.structure]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDNK5X0C9_gj"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# This is a very simple data generator to simplify some of the model training below, notably it deals with the variable sequence length by using a batch size of 1\n",
        "class basicGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, shuffle=True, batch_size=1):\n",
        "      self.df = df\n",
        "      self.shuffle = shuffle\n",
        "      self.batch_size = batch_size\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1.0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = np.arange(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        batch_input = []\n",
        "        batch_target = []\n",
        "        for i in indexes:\n",
        "            amino_acid_sequence = self.df.amino_acid_index.iloc[i]\n",
        "            label_sequence = self.df.structure_index.iloc[i]\n",
        "            batch_input.append(amino_acid_sequence)\n",
        "            batch_target.append(label_sequence)\n",
        "\n",
        "        batch_input = np.stack(batch_input)\n",
        "        batch_target = np.array(batch_target)\n",
        "\n",
        "        return batch_input, batch_target\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGAlvDB3_Gep"
      },
      "source": [
        "# Instantiate our generators for the train and test set\n",
        "train_generator = basicGenerator(df_train_raw)\n",
        "test_generator = basicGenerator(df_test_raw)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRNIT1dh90gx"
      },
      "source": [
        "Here we test a toy model to confirm that our data loading and formatting has worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d85VeJeR1z8_"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# this is pretty close to a minimal sequence model, arguably for a true minimal model the RNN should use a simpler cell than LSTM\n",
        "input = tf.keras.layers.Input((None, 1))\n",
        "output = tf.keras.layers.LSTM(3, return_sequences=True, activation='softmax')(input)\n",
        "\n",
        "toy_model = tf.keras.models.Model(inputs=[input], outputs=[output])\n",
        "toy_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ3PMdAM3blm"
      },
      "source": [
        "# fit our model, empirically 10 epochs is about right for the training to plateau\n",
        "toy_model.fit(train_generator, validation_data=test_generator, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHKC51f_Y4Fo",
        "outputId": "227899fd-eae9-4f2b-cbfa-9f45b2d4e500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# there's a lot of list comprehension here, but the end result is simple: a single list of every amino acid and a corresponding single list of every prediction\n",
        "predictions = [toy_model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.99      0.70      1923\n",
            "           1       1.00      0.00      0.00       849\n",
            "           2       0.28      0.01      0.02       748\n",
            "\n",
            "    accuracy                           0.54      3520\n",
            "   macro avg       0.61      0.33      0.24      3520\n",
            "weighted avg       0.60      0.54      0.39      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xxc-lLJamcp"
      },
      "source": [
        "Unsurprisingly, our performance on the validation set is essentially equivalent to the majority baseline (i.e. guessing no structure for every amino acid which occurs 54.6% of the time). Now comes the fun part where we move to a real model and start pushing our performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgXp8Y6xZp1G"
      },
      "source": [
        "# Add a randomly initialized embedding layer to our model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.LSTM(3, return_sequences=True, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNHEkO5rlvvw"
      },
      "source": [
        "# Train a fair bit longer since it's actually learning something\n",
        "model.fit(train_generator, validation_data=test_generator, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUp7GbYFnAJL",
        "outputId": "5801b1f9-8a12-4da4-be28-88223e355e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate updated performance metrics\n",
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.87      0.71      1923\n",
            "           1       0.42      0.26      0.32       849\n",
            "           2       0.45      0.10      0.17       748\n",
            "\n",
            "    accuracy                           0.56      3520\n",
            "   macro avg       0.49      0.41      0.40      3520\n",
            "weighted avg       0.52      0.56      0.50      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhXsaU2nKPO"
      },
      "source": [
        "Just adding in the randomly initialized embedding layer has moved us to a model that's beginning to train meaningfully. The overall accuracy is only slightly improved, but the macro f1 is massively improved as the model now makes an attempt at guessing the minority classes.\n",
        "\n",
        "The plateauing of the model performance on the train set indicates that we don't have sufficient model complexity (or sufficiently rich inputs) to to fit this data. Expanding model complexity is easier than enriching the data so we'll start there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKhIOrxUFZ_a"
      },
      "source": [
        "# Add a single dense layer after the LSTM that operates independently on each time step (essentially like adding another RNN layer with no memory)\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu'))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTvkw2NFxfD"
      },
      "source": [
        "# Same training regimen as above\n",
        "model.fit(train_generator, validation_data=test_generator, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eek8WEemNC4N",
        "outputId": "15a3981a-ccf6-4b16-ccb2-a7181920d52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate updated performance metrics\n",
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.80      0.70      1923\n",
            "           1       0.43      0.37      0.40       849\n",
            "           2       0.42      0.19      0.26       748\n",
            "\n",
            "    accuracy                           0.57      3520\n",
            "   macro avg       0.49      0.45      0.45      3520\n",
            "weighted avg       0.54      0.57      0.54      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V12WxutPiVc"
      },
      "source": [
        "We've got more overfitting with the extra capacity (not seen in the above test set performance), but not a significant performance increase. We can enrich the input data simply by swapping to a biLSTM. This allows each prediction to look at the following amino acids in addition to the prior amino acids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmfqVBwNP2Ut"
      },
      "source": [
        "# Very easy to swap to biLSTM with just the Bidirectional layer wrapping the LSTM\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu')))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ0EEzfiQGB9"
      },
      "source": [
        "# Increase the training time since it is continuing to learn longer\n",
        "model.fit(train_generator, validation_data=test_generator, epochs=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53QQN9vIRpFd",
        "outputId": "a57c2d4c-be7a-4431-8fca-4aaed23913c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate updated performance metrics\n",
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "no_structure       0.64      0.85      0.73      1923\n",
            " alpha_helix       0.49      0.37      0.42       849\n",
            "  beta_sheet       0.46      0.21      0.29       748\n",
            "\n",
            "    accuracy                           0.60      3520\n",
            "   macro avg       0.53      0.48      0.48      3520\n",
            "weighted avg       0.57      0.60      0.56      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHc-UTF0UWEk"
      },
      "source": [
        "We've got another small increase from the biLSTM and could likely chase a significantly larger increase with refinement of the architecture and training regime. At this point we can see if our embeddings are capturing meaningful properties of the amino acids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLimB6jUwAl"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use a pca (primary component analysis) to reduce the dimensionality for visual inspection\n",
        "pca = PCA(2)\n",
        "embedding_array = model.layers[0].get_weights()[0]\n",
        "embedding_2d = pca.fit_transform(embedding_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3rX2wAZVctf",
        "outputId": "86c023b6-ae48-4e2e-f401-4e306f6968db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the embedding for each amino acid\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=3)\n",
        "for key in amino_acid_to_index:\n",
        "  plt.annotate(key, (embedding_2d[amino_acid_to_index[key], 0], embedding_2d[amino_acid_to_index[key], 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4VWWeXaRack"
      },
      "source": [
        "The answer to the above question is by and large, no, not really. One can argue for certain points such as proline, an unusual amino acid, being isolated, but it's a bit like reading tea leaves with this lack of structure. This is an excellent argument that adding in the biophysical properties will be useful. \n",
        "\n",
        "Primarily because it's interesting, we're going to inject this knowledge by using it as the initial embeddings rather than random initialization. An alternative, more straightforward method would to just concat these properties onto the output of the embedding layer.\n",
        "\n",
        "We'll take the most predictive features for a vaguely related task referenced here https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2186325/ in table 2. We'll leave the 4th and last embedding dimension as randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD9m_Lh_VfMA"
      },
      "source": [
        "amino_acid_to_volume = {'A': 88.6, 'C': 108.5, 'D':  111.1, 'E': 138.4, 'F': 189.9, 'G': 60.1, 'H': 153.2, 'I': 166.7, 'K': 168.6, 'L': 166.7, 'M': 162.9, 'N': 114.1, 'P': 112.7, 'Q': 143.8, 'R': 173.4, 'S': 89.0, 'T':116.1, 'V': 140.0, 'W': 227.8, 'Y': 193.6}\n",
        "amino_acid_to_hydrophobicity = {'A': 1.8, 'C': 2.5, 'D':  -3.5, 'E': -3.5, 'F': 2.8, 'G': -0.4, 'H': -3.2, 'I': 3.8, 'K': -3.9, 'L': 3.8, 'M': 1.9, 'N': -3.5, 'P': -1.6, 'Q': -3.5, 'R': -4.5, 'S': -0.8, 'T':-0.7, 'V': 4.2, 'W': -0.9, 'Y': -1.3}\n",
        "amino_acid_to_isoelectric = {'A': 6.00, 'C': 5.05, 'D':  2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, 'H': 7.47, 'I': 5.94, 'K': 9.59, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.3, 'Q': 5.65, 'R': 11.15, 'S': 5.68, 'T':5.64, 'V': 5.96, 'W': 5.89, 'Y': 5.66}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Ls7FNdYQbj"
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# We scale these properties to mean=0 and std=1 for faster/smoother training\n",
        "embedding_biophysical_properties = scale(np.array(([amino_acid_to_hydrophobicity[x] for x in unique_amino_acids_in_train], \n",
        "                                             [amino_acid_to_volume[x] for x in unique_amino_acids_in_train],\n",
        "                                             [amino_acid_to_isoelectric[x] for x in unique_amino_acids_in_train])).transpose())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I60GhlV_Y51B"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu')))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# this is a quick workaround to partially modify the embedding matrix so that the first 3 dimensions match the biophysical properties\n",
        "embedding_array = model.layers[0].get_weights()[0]\n",
        "embedding_array[:, :3] = embedding_biophysical_properties\n",
        "model.layers[0].set_weights([embedding_array])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiwWR52JZUSh"
      },
      "source": [
        "model.fit(train_generator, validation_data=test_generator, epochs=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri1OjV3keq2y",
        "outputId": "14014017-aa08-44b4-aa08-1dd9ccc996c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "no_structure       0.64      0.85      0.73      1923\n",
            " alpha_helix       0.53      0.39      0.45       849\n",
            "  beta_sheet       0.53      0.24      0.33       748\n",
            "\n",
            "    accuracy                           0.61      3520\n",
            "   macro avg       0.57      0.49      0.50      3520\n",
            "weighted avg       0.59      0.61      0.58      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlksaImdDLy"
      },
      "source": [
        "We've made progress, but it continues to be very slight. Let's see if our embeddings have a more discernable structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUmjJxkJcp_k",
        "outputId": "b64847eb-6c0a-4b8a-dfce-a4f017afdbad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use a pca (primary component analysis) to reduce the dimensionality for visual inspection\n",
        "pca = PCA(2)\n",
        "embedding_array = model.layers[0].get_weights()[0]\n",
        "embedding_2d = pca.fit_transform(embedding_array)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the embedding for each amino acid\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=3)\n",
        "for key in amino_acid_to_index:\n",
        "  plt.annotate(key, (embedding_2d[amino_acid_to_index[key], 0], embedding_2d[amino_acid_to_index[key], 1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rehuo7LifBwm"
      },
      "source": [
        "There's still a strong element of reading tea leaves, but one could argue that the hydrophobic amino acids have mostly segregated from the charged/polarizable side chains. Similarly, the positively charge amino acids are well separated from the rest. Considering the marginal gain in accuracy it's unsurprising that we still lack a clear structure here.\n",
        "\n",
        "At this point we have a few options to push forward:\n",
        "\n",
        "\n",
        "*   Hyperparameter tuning would like get several points of performance\n",
        "*   Pretraining our embeddings with an extra dataset of just unlabeled amino acid sequences\n",
        "*   Chunking our proteins into fixed length sequences\n",
        "\n",
        "Since we're within a very small margin of our goal of 62.7%, I'll try some architecture/hyperparameter tuning to squeeze out performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwpWHZSoffxH"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu')))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(4, activation='relu')))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1E-3), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# this is a quick workaround to partially modify the embedding matrix so that the first 3 dimensions match the biophysical properties\n",
        "embedding_array = model.layers[0].get_weights()[0]\n",
        "embedding_array[:, :3] = embedding_biophysical_properties\n",
        "model.layers[0].set_weights([embedding_array])\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTEmTOpBAJA8"
      },
      "source": [
        "model.fit(train_generator, validation_data=test_generator, epochs=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrK5gh2HC4ks",
        "outputId": "6043c819-6ee7-41fd-f74b-f3b2285df238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "no_structure       0.67      0.85      0.75      1923\n",
            " alpha_helix       0.52      0.41      0.46       849\n",
            "  beta_sheet       0.50      0.28      0.36       748\n",
            "\n",
            "    accuracy                           0.62      3520\n",
            "   macro avg       0.56      0.51      0.52      3520\n",
            "weighted avg       0.60      0.62      0.59      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwGiwnNsPKuR"
      },
      "source": [
        "That small change is enough to replicate the 62.7% performance (within the epoch to epoch variation). The original paper asserts that this is a likely ceiling on performance with this dataset so we'll pause here and call it a win.\n",
        "\n",
        "With 30 years of additional research in deep learning we might expect to not only be able to quickly replicate this performance, but in fact significantly outperform it. My approach here, while on it's surface quite different, shares some deeper similarities with the original sliding window multilayer perceptron. The LSTM despite it's best efforts is not going to meaningfully learn long distance dependencies (see all the recent work with transformers to overcome this) so it is still only capturing local properties of the sequence. Under this view, the bidirectional approach is equivalent to the sliding window being centered on the target amino acid, providing past and future information. It is therefore perhaps unsurprising that it results in very similar performance.\n",
        "\n",
        "To leverage modern deep learning to go beyond this, I see two clear avenues: pretraining better embeddings and moving to an attention-based architecture such as a transformer. There's a vast corpus of work showing the value of pretraining embeddings now that we have the computational resources to work with very large, unlabeled datasets. Moving to something like a transformer has a decent shot at capturing the long distance dependencies that we know are pivotal to protein structure (conformational changes are a great example of this)."
      ]
    }
  ]
}