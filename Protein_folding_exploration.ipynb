{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein_folding_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpfYjtOzWSg5AMJKYy792z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcummer/deep_learning_short_projects/blob/main/Protein_folding_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDhPFRLAanB"
      },
      "source": [
        "**Very initial work**\n",
        "\n",
        "This is an exploratory project to play around with applying various NLP techniques to predict the secondary structure of a protein from the amino acid sequence. Right now, I just have the foundation of loading the data and training a toy model. \n",
        "\n",
        "There's a lot of directions to take this with likely highlights being deploying a proper NLP model like a biLSTM with randomly initialized embeddings for the amino acids, exploring those embeddings to see if they capture the similarities we'd expect i.e. arginine and lysine are close in that space, and incorporating biophysical properties of the amino acids such as electronegativity and flags for special amino acids such as proline.\n",
        "\n",
        "We won't worry about optimizing the details of model architecture and training for the most part. If we end up with something particularly high performing we may dig into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK-hBf0ndpvf"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.train\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/protein-secondary-structure/protein-secondary-structure.test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qbPeYxeTwn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def parse_to_df(path):\n",
        "  with open(path,'r') as f:\n",
        "    content = f.readlines()\n",
        "  df = pd.DataFrame()\n",
        "  protein_count = 0\n",
        "  amino_acids = []\n",
        "  structure = []\n",
        "  for i in content:\n",
        "    i = i.strip()\n",
        "    if 'end' in i:\n",
        "        df = df.append(pd.DataFrame({'amino_acid':[amino_acids], 'structure':[structure], 'protein_count':[protein_count]}))\n",
        "        protein_count += 1\n",
        "        amino_acids = []\n",
        "        structure = []\n",
        "    elif len(i) == 3:\n",
        "      amino_acids.append(i.split(' ')[0])\n",
        "      structure.append(i.split(' ')[1])\n",
        "  return df"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYiBkP1H9cU9"
      },
      "source": [
        "Here we load the text files for the train and test splits into easily accessed dataframes. We also also assign an index to each amino acid and structure to replace the text character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcdRguigfH6z"
      },
      "source": [
        "df_train_raw = parse_to_df('/content/protein-secondary-structure.train')\n",
        "df_test_raw = parse_to_df('/content/protein-secondary-structure.test')\n",
        "\n",
        "\n",
        "unique_amino_acids_in_train = np.unique([item for sublist in df_train_raw.amino_acid for item in sublist])\n",
        "unique_amino_acids_in_test = np.unique([item for sublist in df_test_raw.amino_acid for item in sublist])\n",
        "[unique_amino_acids_in_train.append(x) for x in unique_amino_acids_in_test if x not in unique_amino_acids_in_train]\n",
        "\n",
        "amino_acid_to_index = {}\n",
        "i=0\n",
        "for x in unique_amino_acids_in_train:\n",
        "  amino_acid_to_index[x] = i\n",
        "  i += 1\n",
        "\n",
        "\n",
        "structure_to_index = {'_': 0, 'h': 1, 'e': 2}\n",
        "\n",
        "df_train_raw['amino_acid_index'] = [[amino_acid_to_index[x] for x in y] for y in df_train_raw.amino_acid]\n",
        "df_test_raw['amino_acid_index'] = [[amino_acid_to_index[x] for x in y] for y in df_test_raw.amino_acid]\n",
        "df_train_raw['structure_index'] = [[structure_to_index[x] for x in y] for y in df_train_raw.structure]\n",
        "df_test_raw['structure_index'] = [[structure_to_index[x] for x in y] for y in df_test_raw.structure]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDNK5X0C9_gj"
      },
      "source": [
        "class basicGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, shuffle=True, batch_size=1):\n",
        "      self.df = df\n",
        "      self.shuffle = shuffle\n",
        "      self.batch_size = batch_size\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1.0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = np.arange(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        batch_input = []\n",
        "        batch_target = []\n",
        "        for i in indexes:\n",
        "            amino_acid_sequence = self.df.amino_acid_index.iloc[i]\n",
        "            label_sequence = self.df.structure_index.iloc[i]\n",
        "            batch_input.append(amino_acid_sequence)\n",
        "            batch_target.append(label_sequence)\n",
        "\n",
        "        batch_input = np.stack(batch_input)\n",
        "        batch_target = np.array(batch_target)\n",
        "\n",
        "        return batch_input, batch_target\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGAlvDB3_Gep"
      },
      "source": [
        "train_generator = basicGenerator(df_train_raw)\n",
        "test_generator = basicGenerator(df_test_raw)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRNIT1dh90gx"
      },
      "source": [
        "Here we test a toy model to confirm that our data loading and formatting has worked as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d85VeJeR1z8_"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input = tf.keras.layers.Input((None, 1))\n",
        "output = tf.keras.layers.LSTM(3, return_sequences=True, activation='softmax')(input)\n",
        "\n",
        "toy_model = tf.keras.models.Model(inputs=[input], outputs=[output])\n",
        "toy_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ3PMdAM3blm"
      },
      "source": [
        "toy_model.fit(train_generator, validation_data=test_generator, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHKC51f_Y4Fo",
        "outputId": "227899fd-eae9-4f2b-cbfa-9f45b2d4e500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = [toy_model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.99      0.70      1923\n",
            "           1       1.00      0.00      0.00       849\n",
            "           2       0.28      0.01      0.02       748\n",
            "\n",
            "    accuracy                           0.54      3520\n",
            "   macro avg       0.61      0.33      0.24      3520\n",
            "weighted avg       0.60      0.54      0.39      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xxc-lLJamcp"
      },
      "source": [
        "Unsurprisingly, our performance on the validation set is essentially equivalent to the majority baseline (i.e. guessing no structure for every amino acid). Now comes the fun part where we move to a real model and start pushing our performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgXp8Y6xZp1G"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.LSTM(3, return_sequences=True, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNHEkO5rlvvw"
      },
      "source": [
        "model.fit(train_generator, validation_data=test_generator, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUp7GbYFnAJL",
        "outputId": "5801b1f9-8a12-4da4-be28-88223e355e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.87      0.71      1923\n",
            "           1       0.42      0.26      0.32       849\n",
            "           2       0.45      0.10      0.17       748\n",
            "\n",
            "    accuracy                           0.56      3520\n",
            "   macro avg       0.49      0.41      0.40      3520\n",
            "weighted avg       0.52      0.56      0.50      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhXsaU2nKPO"
      },
      "source": [
        "Just adding in the randomly initialized embedding layer has moved us to a model that's beginning to train meaningfully. The overall accuracy is only slightly improved, but the macro f1 is massively improved as the model now makes an attempt at guessing the minority classes.\n",
        "\n",
        "The plateauing of the model performance on the train set indicates that we don't have sufficient model complexity (or sufficiently rich inputs) to to fit this data. Expanding model complexity is easier than enriching the data so we'll start there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKhIOrxUFZ_a"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu'))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTvkw2NFxfD"
      },
      "source": [
        "model.fit(train_generator, validation_data=test_generator, epochs=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eek8WEemNC4N",
        "outputId": "15a3981a-ccf6-4b16-ccb2-a7181920d52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.80      0.70      1923\n",
            "           1       0.43      0.37      0.40       849\n",
            "           2       0.42      0.19      0.26       748\n",
            "\n",
            "    accuracy                           0.57      3520\n",
            "   macro avg       0.49      0.45      0.45      3520\n",
            "weighted avg       0.54      0.57      0.54      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V12WxutPiVc"
      },
      "source": [
        "We've got more overfitting with the extra capacity, but not a significant performance increase. We can now enrich the input data simply by swapping to a biLSTM. This allows each prediction to look at the following amino acids in addition to the prior amino acids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmfqVBwNP2Ut"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(unique_amino_acids_in_train), 4))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True, activation='relu')))\n",
        "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3, activation='softmax')))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ0EEzfiQGB9"
      },
      "source": [
        "model.fit(train_generator, validation_data=test_generator, epochs=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53QQN9vIRpFd",
        "outputId": "a57c2d4c-be7a-4431-8fca-4aaed23913c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = [model.predict(x) for x in test_generator]\n",
        "print(classification_report([z for _, y in test_generator for x in y for z in x], [np.argmax(z) for y in predictions for x in y for z in x], target_names=['no_structure', 'alpha_helix', 'beta_sheet']))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "no_structure       0.64      0.85      0.73      1923\n",
            " alpha_helix       0.49      0.37      0.42       849\n",
            "  beta_sheet       0.46      0.21      0.29       748\n",
            "\n",
            "    accuracy                           0.60      3520\n",
            "   macro avg       0.53      0.48      0.48      3520\n",
            "weighted avg       0.57      0.60      0.56      3520\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHc-UTF0UWEk"
      },
      "source": [
        "We've got a moderate increase from the biLSTM and could likely chase a significantly larger increase with refinement of the architecture and training regime. At this point our model is sufficiently accurate to be worth looking at the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLimB6jUwAl"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(2)\n",
        "embedding_array = model.layers[0].get_weights()[0]\n",
        "embedding_2d = pca.fit_transform(embedding_array)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3rX2wAZVctf",
        "outputId": "4850f4b0-a241-416b-cb5d-aac013ac4159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1])"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f1051122ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD9m_Lh_VfMA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}